---
title: "Motion Tracking with Tracktor and OpenCV in R"
author: "Mike O'Brien"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_notebook
---

Everything below assumes that you have successfully installed Python and the [NumPy](http://www.numpy.org/), [pandas](https://pandas.pydata.org/), [SciPy](https://scipy.org/scipylib/index.html), [matplotlib](https://matplotlib.org/), [scikit-learn](https://scikit-learn.org/stable/), [OpenCV](https://docs.opencv.org/master/), and [Tracktor](https://github.com/vivekhsridhar/Tracktor) packages.

## Using R
I'm a heavy R user, and way more used to syntax and scripting in R than I am in Python and using, say, [Jupyter notebooks](http://jupyter.org/). I know for a fact that there are faster ways to run this, specifically using programs that are optimized for Python. Regardless, I am going to use [R](https://www.r-project.org/), [RStudio](https://www.rstudio.com/), and the [`reticulate` package](https://rstudio.github.io/reticulate/) to interface with the code needed.

## Writing the Python script
Tracktor ships with a suite of Jupyter notebooks that run a few examples, and they suggest that you manipulate the parameters in the notebook to suit your own devices. I'm just converting these to regular, run-of-the-mill scripts I can source through R.

To start this process, I load `reticulate` in R and force it to use our previously-created condaenv.
```{r setup}
library(reticulate)
use_condaenv('fish-video', required = T)
```

## Loading the Python packages
I'm going to load all of the packages into the Python session. All code from here on out will be in Python. If you have [RStudio 1.2+](https://www.rstudio.com/products/rstudio/download/preview/), this will automatically run in RStudio.

```{python}
import numpy as np
import pandas as pd
import tracktor as tr
import cv2
import sys
import scipy.signal
from matplotlib.pyplot import imshow
from scipy.optimize import linear_sum_assignment
from scipy.spatial.distance import cdist
```

## Setting up some variables
**Nearly everything below has been adapted from Tracktor's ["termite collective behavior"](https://github.com/vivekhsridhar/tracktor/blob/master/examples/3d%20Termite_collective_behaviour.ipynb) example. Much is taken verbatim from their guide. All credit to them.**

* `n_inds` is the number of individuals
* `t_id` is fish id and is also used for individual identification
* `colors` is a vector of BGR values which are used to identify individuals in the video
* Mike note: the following bullets were also included in the guide, but I didn't find that they were 100% necessary.
   * The number of elements in `colors` should be greater than `n_inds` (**this is necessary for visualization only**)
   * The number of elements in `t_id` should be greater than `n_inds` (**this is necessary to get individual-specific data**)

```{python}
n_inds = 3
t_id = ['A', 'B', 'C']
colors = [(0,0,255),(0,255,255),(255,0,255),(255,255,255),
          (255,255,0),(255,0,0),(0,255,0),(0,0,0)]
```

This is the block_size and offset used for adaptive thresholding (**block_size should always be odd**). These values are critical for tracking performance.
```{python}
block_size = 501
offset = 10
```

### Side note on thresholding
This is really the bread and butter of the whole process. For all intents and purposes, [thresholding](https://docs.opencv.org/3.4.0/d7/d4d/tutorial_py_thresholding.html) can be thought of as drawing a line in the range of color values, whereby anything above the threshold is white, and anything below is black. Basically, you will be turning each frame into a [binary, black or white image](https://docs.opencv.org/3.4.0/threshold.jpg).

[Adaptive thresholding](https://docs.opencv.org/3.4.0/ada_threshold.jpg), the method that Tracktor uses, creates a threshold according to the "neighborhood" of surrounding pixels. This way, if a pixel is darker than the majority of pixels, but lighter than those around it, it will be considered white instead of black. This is necessary when we have variable light conditions.

The `block_size` variable above determines the size, in pixels, of the neighborhood that we're going to average over. Ideally, this would be the size of the fish, but due to variable fish sizes as a result of the depth-of-field, we have to make an educated guess. 

The `offset` variable is just a fudge factor. Increase it if you want to make more pixels white, decrease it if you want to make more pixels black.

You'll need to play around with these values to get it right. I'll outline later how I came to choose these specific values.

### Back to work
The scaling parameter can be used to speed up tracking if video resolution is too high (use value 0-1).
```{python}
scaling = 1.0
```

Minimum area and maximum area occupied by the animal in number of pixels. This parameter is used to get rid of other objects in view that might be hard to threshold out but are differently sized.
```{python}
min_area = 9000
max_area = 70000
```

`mot` determines whether the tracker is being used in noisy conditions to track a single object or for multi-object. Using this will enable k-means clustering to force n_inds number of animals.
```{python}
mot = True
```

Lastly, name of the source video and paths.
```{python}
video = 'bsb_vid'
input_vidpath = 'c:/users/secor/desktop/fish-video/' + video + '.mp4'
output_vidpath = 'c:/users/secor/desktop/fish-video/' + video + '_tracked.mp4'
output_filepath = 'c:/users/secor/desktop/fish-video/' + video + '_tracked.csv'
codec = 'mp4v'
```

## Use OpenCV to track your individuals
First, open the video file for use by OpenCV. We'll also have this throw an error if the video location doesn't exist.
```{python}
cap = cv2.VideoCapture(input_vidpath)
if cap.isOpened() == False:
    sys.exit('Video file cannot be read! Please check input_vidpath to ensure it     is correctly pointing to the video file.')
```

Create the output video with the contour and centroid of tracked object(s).
* `fourcc` makes sure the output has the correct codec
* `output_framesize` stored the frame size of the original video
* `out` writes the output to the proper codec/frame rate/resolution

```{python}
fourcc = cv2.VideoWriter_fourcc(*codec)
output_framesize = (int(cap.read()[1].shape[1] * scaling),
      int(cap.read()[1].shape[0] * scaling))
out = cv2.VideoWriter(filename = output_vidpath, fourcc = fourcc, fps = 60.0,
      frameSize = output_framesize, isColor = True)
```

Preallocate memory for the individual location(s) measured in the last and current step, the data frame to hold locations, and the index to start the coming loop.
```{python}
meas_last = list(np.zeros((n_inds,2)))
meas_now = list(np.zeros((n_inds,2)))

df = []
last = 0
```

And here's the fun stuff!!
```{python, message = F, error = T, warning = F}
while(True):
    # Capture frame-by-frame
    ret, frame = cap.read()

    this = cap.get(1)
    if ret == True:
        # Preprocess the image for background subtraction
        frame = cv2.resize(frame, None, fx = scaling, fy = scaling,
                            interpolation = cv2.INTER_LINEAR)
        imgplot = imshow(frame)
        print(imgplot)
        
        # Thresholding!!!
        thresh = tr.colour_to_thresh(frame, block_size, offset)
        
        # Draw contours
        final, contours, meas_last, meas_now = tr.detect_and_draw_contours(frame,
                                thresh, meas_last, meas_now, min_area, max_area)
        if len(meas_now) != n_inds:
            contours, meas_now = tr.apply_k_means(contours, n_inds, meas_now)

        row_ind, col_ind = tr.hungarian_algorithm(meas_last, meas_now)
        final, meas_now, df = tr.reorder_and_draw(final, colors, n_inds,
                                        col_ind, meas_now, df, mot, this)

        # Create output dataframe
        for i in range(n_inds):
            df.append([this, meas_now[i][0], meas_now[i][1], t_id[i]])

        # Display the resulting frame
        out.write(final)
        cv2.imshow('frame', final)
        if cv2.waitKey(1) == 27:
            break

    if last >= this:
        break

    last = this
```

Write positions to file:
```{python}
df = pd.DataFrame(np.matrix(df), columns = ['frame','pos_x','pos_y', 'id'])
df.to_csv(output_filepath, sep=',')
```

When everything done, release the capture. If the code didn't run to completion but you'd still like to see the results, run this to release everything.
```{python}
cap.release()
out.release()
cv2.destroyAllWindows()
cv2.waitKey(1)
```
